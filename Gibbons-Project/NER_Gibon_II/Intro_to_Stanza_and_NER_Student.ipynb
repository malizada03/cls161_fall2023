{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07043fc9",
   "metadata": {},
   "source": [
    "**Introduction to Stanza and Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75839c76",
   "metadata": {},
   "source": [
    "Link to stanza documentation: https://stanfordnlp.github.io/stanza/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980d6dc",
   "metadata": {},
   "source": [
    "More documentation with examples, geared towards beginners: https://github.com/stanfordnlp/stanza/blob/main/demo/Stanza_Beginners_Guide.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd783f",
   "metadata": {},
   "source": [
    "Optional Reading: https://www.newfireglobal.com/learn/natural-language-understanding-tools/#informed \n",
    "Article that covers some of the main differences between spacy and stanza with examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60838ab",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())  #this shows us the current working directory we are in \n",
    "                    #Printing it is useful for making your eventual file path as we will see below!\n",
    "#make sure that you are in the directory and folder which contains the textfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97aa471",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd NER_Gibbon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b26eb8",
   "metadata": {},
   "source": [
    "**Inputs**\n",
    "Stanza can handle multiple types of inputs for different tasks. For the purposes of this exercise, we are giving it a list of sentences (ie strings).\n",
    "If we weren't splitting the text by sentence ourselves beforehand, we would have to give it just strings (i.e.,the entire text file) and the pipeline would split the strings into sentences itself as part of the tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344deb57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Here, we are loading the stanza model for english language processing \n",
    "#We will go through what these different processors do in a bit!\n",
    "# To prevent output capacity issues, we are pre tokenizing the text before we give it to the processors\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos,lemma, depparse,ner',tokenize_pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses an f string with my current working directory #ands file name\n",
    "#this is an example of when you can use f-strings to your advantage \n",
    "path= f'{(os.getcwd())}\\gibbon_decline_volume1_chap21.txt' \n",
    "with open( path, encoding='utf-8', mode='r') as f:\n",
    "        vol1_chap21 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d756e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol1_chap21 #looking at the text \n",
    "#luckily for us, the format of this file shows every sentence ends with a period and then one white space \n",
    "#so we can use .split to create a list of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chap21_sents= vol1_chap21.split('. ')\n",
    "chap21_sents   #even if it is not perfect, its good enough for our purposes \n",
    "#now we have one large list of every sentence in this chapter! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278ad2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use list indexing to grab a sentence at a time \n",
    "#creating a Doc can take some time, so let's start with a test sentence \n",
    "test=    \n",
    "\n",
    "\n",
    "#pulling a random sentence \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e15d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3f696",
   "metadata": {},
   "outputs": [],
   "source": [
    " #stanza's output is something called a Doc \n",
    "#lets see what the output looks like \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b18c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see what type this is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "##take 5-10 minutes and go to the stanza documentation I have linked above\n",
    "#in a group, write code to access the text, pos, ner, and upos compontents of our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759c921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#next, look at the stanza documentation and print the named entities for our document "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbd404",
   "metadata": {},
   "source": [
    "**NER and processing multiple documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142eea6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a new pipeline with just the NER processor \n",
    "nlp_ner= stanza.Pipeline(lang='en', processors='tokenize, ner',tokenize_pretokenized=True)\n",
    "#pretokenized= True is still included, since we are going to give the processor our large list of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a solution for our prior output issue-- still takes a bit to run though \n",
    "# Note! Can only be done with text that has already been tokenized into sentences\n",
    "#this allows us to parallel process multiple sentences at a time\n",
    "Gibbon_docs_chap21sents = [stanza.Document([], text=d) for d in chap21_sents]\n",
    "Gibbon_out_docs = nlp_ner(Gibbon_docs_chap21sents) \n",
    "print(Gibbon_out_docs[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how we access the named entities from our documents list \n",
    "for doc in Gibbon_out_docs:\n",
    "    print(doc.ents) \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d263a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_Dict= {}\n",
    "for doc in Gibbon_out_docs:\n",
    "    for sent in doc.sentences:\n",
    "        for token in sent.tokens:\n",
    "            if token.ner == 'O':\n",
    "                continue\n",
    "            else:\n",
    "                NER_Dict[token.text]= token.ner\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc0cdb0",
   "metadata": {},
   "source": [
    "**Quick Pandas Dataframes Intro**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f16afb",
   "metadata": {},
   "source": [
    "Beginner Tutorials: https://pandas.pydata.org/docs/getting_started/index.html#getting-started\n",
    "\n",
    "\n",
    "https://access.tufts.edu/udemy-business UDEMY is a great resource for coding tutorials.\n",
    "I especially recommend Python for Data Science and Machine Learning Bootcamp by Jose Portilla (there is a section on pandas for data science analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe251f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fde262",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_frame= pd.DataFrame.from_dict(NER_Dict, orient= 'index')\n",
    "ner_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d38853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a new index of numbers that is not the entity \n",
    "ner_frame.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_frame.rename(columns={'index':'token', 0: 'NER_tag'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_tags= { 'PER':'People, including fictional characters', 'NORP':'Nationalities, religious and political groups: Jewish, Buddhist',\n",
    "           'TIME':'Time shorter than a day','ORG':'Companies, agencies, institutions', 'GPE':'Countries, cities, regions (districts)',\n",
    "'LOC':'Geographical entities', 'PRODUCT':'Objects, vehicles, foods', 'EVENT':'Named battles, wars, sports events, catastrophes',\n",
    "'QUANTITY':'Measurements: 10 kg, 200 km', 'ORDINAL':'Numbers of order: first ,third','CARDINAL': 'Numerals that do not fall under another type',\n",
    "'FAC': 'Buildings, airports, roads', 'LANGUAGE':'Any named language'}\n",
    "NER_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_NER_prefixes= {'B':'Beginning of named entity','I':'Token is inside a named entity','O':'Corresponding word is not an entity','E':'End of named entity','S':'Named entity has only one token/element'}\n",
    "nested_NER_prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62053026",
   "metadata": {},
   "outputs": [],
   "source": [
    "nertags_frame= pd.DataFrame.from_dict(NER_tags, orient= 'index')\n",
    "nertags_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bf150",
   "metadata": {},
   "outputs": [],
   "source": [
    "nertags_frame.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nertags_frame.rename(columns={'index':'tag', 0: 'example'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2df33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bioestags_frame= pd.DataFrame.from_dict(nested_NER_prefixes, orient= 'index')\n",
    "bioestags_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d825a41",
   "metadata": {},
   "source": [
    "After what we have seen today, how would you describe a named entity? \n",
    "Do these named entities tell you anything about the contents of this chapter? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50bd98",
   "metadata": {},
   "source": [
    "In this notebook, what inputs did we give stanza to process?\n",
    "What outputs does stanza produce?\n",
    "How did we access the different components of this output? (ie pos, lemma, ner, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b96c0",
   "metadata": {},
   "source": [
    "Do you have any remaining questions or things that feel unclear after this demo? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
